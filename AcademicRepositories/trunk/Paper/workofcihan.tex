% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{ruler}
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}


\newcommand*{\affaddr}[1]{#1} % No op here. Customize it for different styles.
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}

\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\def\ECCV14SubNumber{***}  % Insert your submission number here

\title{Cihan's Work} % Replace with your title

\titlerunning{Cihan's work with submission ID \ECCV14SubNumber}

\authorrunning{Cihan's work with submission ID \ECCV14SubNumber}

\author{Cihan Sari\affmark[1] \and Albert Ali Salah\affmark[2] \and Alkim Almila Akdag Salah\affmark[3]}

\institute{\affmark[1]System and Control Engineering,\\
\affmark[2]	Department of Computer Engineering,\\
\{cihan.sari, salah\}@boun.edu.tr\\
\affmark[3]Royal Netherlands Academy of Arts and Sciences\\
a.a.akdag@uva.nl}

\maketitle

\begin{abstract}
??????????????????? ??? ???? ??? ????? ???? ??? ???? ??? ?? ? ? ???? ????? ??? ??????????????????? ??? ???? ??? ????? ???? ??? ???? ??? ?? ? ? ???? ????? ??? ??????????????????? ??? ???? ??? ????? ???? ??? ???? ??? ?? ? ? ???? ????? ??? 
\dots
\keywords{??????????????????? ??? ???? ??? ????? ???? ??? ???? ??? ?? ? ? ???? ????? ???}
\end{abstract}


\section{Introduction}


Face detection has been a very interesting task for the society, culture, art, history, computer vision and machine learning.

\section{Related Work}

I \textbf{really} should categorize related works in these three: art-related, methodology related and face recognition related.

\cite{van2015toward}\cite{zhu2012face}\cite{XiongD13}\cite{srinivasan2015computerized}\cite{Crowley14a}\cite{googleImage}\cite{pascal-voc-2012}\cite{ILSVRC15}\cite{agrawal2014analyzing}\cite{mathias2014face}\cite{cheng2015semantically}\cite{lempitsky2010learning}\cite{grosso2012understanding}\cite{ng2012recognizing}\cite{castrillon2013improving}\cite{portraitpainting-npar11}

\section{Methodology}

\subsection{Data Supervisor}\label{[ss:datasupervisor]}


In today's modern world, thanks to the availability of Internet and accessible computational power, there are thousands of databases available for a research scientist. However, vast majority of the databases have been gathered for a single purpose and marked as such. For example, Iris Flower Database\cite{fisher1936use} is a multivariate data set that contains sepal and petal dimensions (width and height) from Iris flowers of three related species. Another example, MNIST\cite{lecun1998mnist} is an image database of handwritten digits which is gathered and tailored together to distinguish between different handwritten digits. These databases are currently used as proof of concept or benchmarking studies for machine learning algorithms. MNIST, along with ImageNet\cite{ILSVRC15}, is one of the most popular benchmark for C-NN implementations from different libraries and tool sets. However, majority of these databases have been gathered for a specific purpose and annotated as such and therefore does not (cannot) contain information in all the aspects on the objects they contain.


\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/data_supervisor/arm}
		\caption{Arm}
		\label{fig:arm}
	\end{subfigure}
	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/data_supervisor/bag}
		\caption{Bag}
		\label{fig:bag}
	\end{subfigure}
	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/data_supervisor/cup}
		\caption{Cup}
		\label{fig:cup}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/data_supervisor/hair}
		\caption{Hair}
		\label{fig:hair}
	\end{subfigure}
	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/data_supervisor/jewelry}
		\caption{Jewelry}
		\label{fig:jewelry}
	\end{subfigure}
	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/data_supervisor/painting}
		\caption{Painting}
		\label{fig:painting}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/data_supervisor/pants}
		\caption{Pants}
		\label{fig:pants}
	\end{subfigure}
	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/data_supervisor/plate}
		\caption{Plate}
		\label{fig:plate}
	\end{subfigure}
	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/data_supervisor/portrait}
		\caption{Portrait}
		\label{fig:portrait}
	\end{subfigure}
	\caption{Supervisor query results on RijksMuseum}\label{fig-dsupervisor}
\end{figure}


Supervisor is developed to address this issue. Data supervisor, it's name would suggest, attempts to annotate the database into categories given to the system via strings. Algorithm flow is given in figure \ref{fig:supervisorflow}. In summary, the algorithm flowchart uses Google image search\cite{googleImage} to retrieve first hundred query image results for the string, as a mean to learn the category. These “supervised” categorical data are then converted to information via C-NN\cite{Chatfield14} feature extraction. Therefore, system gains access to the “supervised” categorical information in C-NN feature space. Linear SVM model\cite{CC01a} is trained to distinguish between the categorical information, using C-NN feature responses. Resulting SVM model is then used over the “unknown” C-NN feature responses from the database to compute probability of each image belonging to a given string and sorted accordingly.

Supervisor is a software framework with roots in Matlab\cite{MATLAB:2014}, MatConvNet\cite{matconvnet}, pretrained C-NN model\cite{Chatfield14} (imagenet-vgg-f) on ImageNet\cite{ILSVRC15} and Qt. Its purpose is to sort any given raw database of images into categories specified by the user inspired largely by In Search of Art by Zisserman\cite{Crowley14a}.

\begin{figure}
	\includegraphics[width=\textwidth]{figures/data_supervisor/flowchart}
	\caption{Supervisor flowchart}
	\label{fig:supervisorflow}
\end{figure}

An example of the data supervisor from Rijksmuseum\cite{rijksmuseum1976tot} is given in Figure \ref{fig-dsupervisor}. One of the main differences from Zisserman's\cite{Crowley14a} implementation is that data supervisor does not have a predefined negative set. However, user can still add broad category names into the list for the sole purpose of concentrating other categories (e.g. stuff, things, etc).









\subsection{Face landmarks with Intraface}

\subsection{Face crop with FisherFaces in OpenCV with IntraFace}
162.936 plain crops from RijksMuseum\cite{rijksmuseum1976tot} with ViolaJones only! Lots of repetition..

Fisherfaces and Intraface landmarking success criteria provides 6807 hits. 

1681 categorized by hand into four: 191 female, 533 male, 446 faces in low quality (or lacking color), 511 garbage.

\subsection{Face alignment and LBP features with LinearSVM}
Gender recognition performance study results with 10k US Adult Faces Database (featured in section \ref{ss-db10k}). Decision forest using Weka\cite{hall2009weka}. 10 fold cross-validation results are given in Table \ref{tab:10kadultPerformance}. Confusion matrix is given in Table \ref{tab:10kadultConfusion}.

\begin{table}
	\centering
	\caption{10k Adult Faces Database performances}\label{tab:10kadultPerformance}
	\hspace{1cm}
	\begin{tabular}{l|rr}
		Correctly Classified Instances & 7997 & 90.6\%\\
		Incorrectly Classified Instances & 827 & 9.37\%\\
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{10k Adult Faces Database Confusion Matrix}\label{tab:10kadultConfusion}
	\hspace{1cm}
	\begin{tabular}{rrl}
		f & m & $\leftarrow$ classified as\\
		3017 & 680 & f = female \\
		147&4980&m= male\\
		& & $\uparrow$ actual
	\end{tabular}
\end{table}

\subsection{Style Transfer}

\section{Rijksmuseum Art Database}
\subsection{Meta data and dates on Rijksmuseum}

\subsection{Face Crop on Rijksmuseum}

\section{Face image databases annotated with gender info}
First step on preparing a gender recognition algorithm is to prepare annotated training data. In most cases, variety in the training set is directly proportional to the algorithm's success. 

General info regarding crop, cleaning and if applicable, annotating will be here.

Each subsection should mention the value of databases and their unique characteristics for our study.

Importance of the annotation\cite{mathias2014face} should be remarked.


\subsection{IMDB and Google Image results}\label{[ss-dbGenderGoogleImages]}

To this end, 250 actor and 250 actress names from IMDB (Internet Movie DataBase) are used to collect male and female photos from Google Image results using part of the Database Sorter\ref{[ss:datasupervisor]}. This method is inspired largely by Jia's work: Learning to classify gender from four million images
\cite{jia2015learning}.

For data preparation, Zhu framework\cite{zhu2012face} is used to locate the faces, find the landmarks and crop to face bounding box. Algorithm results are cleaned by hand for false positives (or reorganized in the case of wrong gender).

\subsection{Labeled faces in the wild}\label{ss-dbLFW}
LFW\cite{LFWTech} face database is used to form another set of gender images. Similar to section \ref{[ss-dbGenderGoogleImages]}, Viola-Jones algorithm is used for face crop.

LFW dataset, as provided, are not categorized by the gender but only the names of the celebrities. Therefore, genderize.io framework is used on the first names of the celebrities to determine their gender. Unfortunately, genderize predictions were not very satisfactory (a little above fifty per cent observed) and face crops are, again, reorganized by hand for wrong gender (and false positives).

13580 total face images. 12873 face images that are gender annotated (2907 female, 9966 male). Landmark extraction successful on 12428 (2839 female, 9589 male).


\subsection{10k US Adult Faces Database}\label{ss-db10k}
10k adult faces\cite{bainbridge2013intrinsic} database contains more than ten thousand images, aligned and landmarked. This database contains neither gender information nor first names like LFW. Hence, first Viola-Jones, then hand organization is performed for labeling.

10176 total face images. 10168 face images that are gender annotated (4360 female, 5800 male). Landmark extraction successful on 8824 (3697 female and 5127 male).

\subsection{Rijkmuseum Art Database}\label{ss-dbRijk}
Painting\cite{rijksmuseum1976tot} image pieces hand-annotated into four sub-categories:

\begin{itemize}
	\item Female (499)
	\item Male (1.006)
	\item Bad Face (11.030)
	\item Trash
\end{itemize}

“Trash” contains false positive face regions which are not actually face or are of very poor quality. “Bad Face” category consists of correct face regions that are poor quality, are not paintings - pieces from masks / statues etc., cartoons or any non-oil painting piece in general.


\section{Whole Packages}

\subsection{Painting Recognizer on Google Images}

Google image result collection similar to section \ref{[ss:datasupervisor]} for portrait images over centuries. An example query in Figure \ref{pr-googleres}.

\begin{figure}
	\centering
	\includegraphics[width=.8\textwidth]{PaintingRecognizer-query_results}
	\caption{Google Image search results}
	\label{pr-googleres}	
\end{figure}

Viola-Jones\cite{viola} face detection is used on the downloaded images for face detection. From the face bounding box, a vector is generated pointing downwards with a distance around one face height to pointing the clothing in the portrait.

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{PaintingRecognizer-matlab_results}
	\caption{Painting Recognizer process}
	\label{pr-process}	
\end{figure}

A small rectangle around the clothing is used for color analysis. Example image given in Figure \ref{pr-process}. 

RGB intensity values are then used to compute five dominant colors per 3 years window per gender using k-means (with $k=5$). The representation of the data over time can be seen in Figure \ref{pr-result}.

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{PaintingRecognizer-results}
	\caption{Painting Recognizer results}
	\label{pr-result}	
\end{figure}


\section{Conclusions}

\clearpage

\bibliographystyle{splncs}
\bibliography{workofcihan}
\end{document}
